{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Curse of Dimensionality - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, you'll conduct some mathematical simulations to further investigate consequences of the curse of dimensionality.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Define a Euclidean Distance Function for n-dimensional space\n",
    "* Plot a graph displaying how sparsity increases with n for n-dimensional spaces\n",
    "* Demonstrate how training time increases exponentially as the number of features increases for supervised learning algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparseness in n-Dimensional Space\n",
    "\n",
    "As discussed, points in n-dimensional space become increasingly sparse as the number of dimensions increases. To demonstrate this, you'll write a function to calculate the euclidean distance between two points. From there, you'll then generate random points in n-dimensional space, calculate their average distance from the origin, and plot the relationship between this average distance and n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean Distance\n",
    "\n",
    "To start, write a function which takes two points, p1 and p2, and returns the Euclidean distance between them. Recall that the Euclidean distance between two points is given by:  \n",
    "\n",
    "$$ d(a,b) = \\sqrt{(a_1 - b_1)^2 + (a_2 - b_2)^2 + ... + (a_n - b_n)^2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def euclidean_distance(p1, p2):\n",
    "    square_arr = [ (ai - bi)**2 for ai, bi in zip(p1,p2)]\n",
    "    return np.sqrt(np.sum(square_arr))\n",
    "\n",
    "euclidean_distance([1,2,3,4], [1,0,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Distance From the Origin\n",
    "\n",
    "To examine the curse of dimensionality, you'll investigate the average distance to the center of n-dimensional space. As you'll see, this average distance increases as the number of dimensions increases. To investigate this, generate 100 random points for various n-dimensional spaces. Investigate n-dimensional spaces from n=1 to n=1000. In each of these, construct the 100 random points using a random number between -10 and 10 for each dimension of the point. From there, calculate the average distance from each of these points to the origin. Finally, plot this relationship on a graph; the x-axis will be the n, the number of dimensions, and the y-axis will be the average distance from the origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.63143535, -2.31570945, -5.0639512 ,  3.60425101,  3.12945044,\n",
       "        2.71129579, -3.02191396, -7.90213402, -8.71408255, -1.98075732])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.uniform(-10,10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (1,) and (9,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a470f2c552fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mavg_distances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances_to_origin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_distances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/learn-env/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2811\u001b[0m     return gca().plot(\n\u001b[1;32m   2812\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2813\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/learn-env/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1808\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1810\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m/opt/conda/envs/learn-env/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1611\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1612\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/learn-env/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/learn-env/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/learn-env/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 231\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1,) and (9,)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADh5JREFUeJzt3FGInXeZx/HvyWRdyTp2wJwLSeJS2PTBGISysXHxwi7tRSKS3BS3KYXtGiuyG0XaChWFLfWm27JILqIrm5VaLxpSLzRI3ezFtijSSq3ai7Y8EGJrphHajk2hBK0h716c032HcZLzZuacM3Ge7wcC857zvO95+nTmN2/+ed+31zQNkqT1b8NaNyBJmg4DX5KKMPAlqQgDX5KKMPAlqQgDX5KK2DiqICK+DXwSeDUzdy7zfg84DHwCOA/ckZm/GHejkqTV6XKG/zCw5zLv7wW2D/98Fvjm6tuSJI3byMDPzB8Dv7tMyX7gkcxsMvNpYC4i3j+uBiVJ4zFySaeDLcCZRdvzw9d+e7mdmqbxJt+hXg+cxYCzaDmLlrNobdjQex3or2TfcQR+b5nXRv6vaRpYWHhrDB//529ubhPnzp1f6zauCs6i5SxazqLV78++vNJ9x3GVzjywbdH2VuDsGI4rSRqjcZzhnwAORcQxYDfwZmZedjlHkjR9XS7LfBS4EdgcEfPAvwJ/AZCZ/wE8zuCSzFMMLsv8p0k1K0lauZGBn5kHRrzfAP8yto4kSRPhnbaSVISBL0lFGPiSVISBL0lFGPiSVISBL0lFGPiSVISBL0lFGPiSVISBL0lFGPiSVISBL0lFGPiSVISBL0lFGPiSVISBL0lFGPiSVISBL0lFGPiSVISBL0lFGPiSVISBL0lFGPiSVISBL0lFGPiSVISBL0lFGPiSVISBL0lFGPiSVISBL0lFGPiSVISBL0lFGPiSVISBL0lFGPiSVMTGLkURsQc4DMwARzPzgSXvfwD4DjA3rLk3Mx8fc6+SpFUYeYYfETPAEWAvsAM4EBE7lpR9FTiemdcDtwLfGHejkqTV6bKkcwNwKjNPZ+bbwDFg/5KaBnjv8OtrgLPja1GSNA5dlnS2AGcWbc8Du5fU3Af8T0R8Hvgr4OZRB+31YG5uU8c217eZmQ3OYshZtJxFy1mMR5fA7y3zWrNk+wDwcGb+e0T8HfDdiNiZmRcvddCmgXPnzl9Bq+vX3NwmZzHkLFrOouUsWv3+7Ir37bKkMw9sW7S9lT9dsjkIHAfIzKeAdwObV9yVJGnsugT+M8D2iLg2It7F4B9lTyyp+Q1wE0BEfJBB4L82zkYlSaszMvAz8wJwCDgJvMjgapznI+L+iNg3LLsbuDMingMeBe7IzKXLPpKkNdRrmrXJ5YsXm2Zh4a01+eyrjeuTLWfRchYtZ9Hq92efBXatZF/vtJWkIgx8SSrCwJekIgx8SSrCwJekIgx8SSrCwJekIgx8SSrCwJekIgx8SSrCwJekIgx8SSrCwJekIgx8SSrCwJekIgx8SSrCwJekIgx8SSrCwJekIgx8SSrCwJekIgx8SSrCwJekIgx8SSrCwJekIgx8SSrCwJekIgx8SSrCwJekIgx8SSrCwJekIgx8SSrCwJekIgx8SSpiY5eiiNgDHAZmgKOZ+cAyNZ8C7gMa4LnMvG2MfUqSVmnkGX5EzABHgL3ADuBAROxYUrMd+DLwscz8EPDFCfQqSVqFLks6NwCnMvN0Zr4NHAP2L6m5EziSmW8AZOar421TkrRaXZZ0tgBnFm3PA7uX1FwHEBE/ZbDsc19m/vflDtrrwdzcpitodf2amdngLIacRctZtJzFeHQJ/N4yrzXLHGc7cCOwFfhJROzMzHOXOmjTwLlz57v2ua7NzW1yFkPOouUsWs6i1e/PrnjfLks688C2RdtbgbPL1PwgM/+Ymb8GksEvAEnSVaJL4D8DbI+IayPiXcCtwIklNd8H/h4gIjYzWOI5Pc5GJUmrMzLwM/MCcAg4CbwIHM/M5yPi/ojYNyw7CSxExAvAE8CXMnNhUk1Lkq5cr2mWLsdPx8WLTbOw8NaafPbVxvXJlrNoOYuWs2j1+7PPArtWsq932kpSEQa+JBVh4EtSEQa+JBVh4EtSEQa+JBVh4EtSEQa+JBVh4EtSEQa+JBVh4EtSEQa+JBVh4EtSEQa+JBVh4EtSEQa+JBVh4EtSEQa+JBVh4EtSEQa+JBVh4EtSEQa+JBVh4EtSEQa+JBVh4EtSEQa+JBVh4EtSEQa+JBVh4EtSEQa+JBVh4EtSEQa+JBVh4EtSEQa+JBVh4EtSERu7FEXEHuAwMAMczcwHLlF3C/AY8JHM/PnYupQkrdrIM/yImAGOAHuBHcCBiNixTN0s8AXgZ+NuUpK0el2WdG4ATmXm6cx8GzgG7F+m7mvAg8Dvx9ifJGlMuizpbAHOLNqeB3YvLoiI64FtmfnDiLinywf3ejA3t6lzo+vZzMwGZzHkLFrOouUsxqNL4PeWea1554uI2AB8HbjjSj64aeDcufNXssu6NTe3yVkMOYuWs2g5i1a/P7vifbss6cwD2xZtbwXOLtqeBXYCT0bES8BHgRMRsWvFXUmSxq7LGf4zwPaIuBZ4BbgVuO2dNzPzTWDzO9sR8SRwj1fpSNLVZeQZfmZeAA4BJ4EXgeOZ+XxE3B8R+ybdoCRpPHpN04yumoCLF5tmYeGtNfnsq43rky1n0XIWLWfR6vdnnwVWtGTunbaSVISBL0lFGPiSVISBL0lFGPiSVISBL0lFGPiSVISBL0lFGPiSVISBL0lFGPiSVISBL0lFGPiSVISBL0lFGPiSVISBL0lFGPiSVISBL0lFGPiSVISBL0lFGPiSVISBL0lFGPiSVISBL0lFGPiSVISBL0lFGPiSVISBL0lFGPiSVISBL0lFGPiSVISBL0lFGPiSVISBL0lFbOxSFBF7gMPADHA0Mx9Y8v5dwGeAC8BrwKcz8+Ux9ypJWoWRZ/gRMQMcAfYCO4ADEbFjSdkvgV2Z+WHge8CD425UkrQ6Xc7wbwBOZeZpgIg4BuwHXninIDOfWFT/NHD7OJuUJK1el8DfApxZtD0P7L5M/UHgR6MO2uvB3NymDh+//s3MbHAWQ86i5SxazmI8ugR+b5nXmuUKI+J2YBfw8VEHbRo4d+58h49f/+bmNjmLIWfRchYtZ9Hq92dXvG+XwJ8Hti3a3gqcXVoUETcDXwE+npl/WHFHkqSJ6BL4zwDbI+Ja4BXgVuC2xQURcT3wLWBPZr469i4lSas28iqdzLwAHAJOAi8CxzPz+Yi4PyL2DcseAt4DPBYRv4qIExPrWJK0Ir2mWXY5fuIuXmyahYW31uSzrzauT7acRctZtJxFq9+ffZbBv5VeMe+0laQiDHxJKsLAl6QiDHxJKsLAl6QiDHxJKsLAl6QiDHxJKsLAl6QiDHxJKsLAl6QiDHxJKsLAl6QiDHxJKsLAl6QiDHxJKsLAl6QiDHxJKsLAl6QiDHxJKsLAl6QiDHxJKsLAl6QiDHxJKsLAl6QiDHxJKsLAl6QiDHxJKsLAl6QiDHxJKsLAl6QiDHxJKsLAl6QiDHxJKsLAl6QiNnYpiog9wGFgBjiamQ8sef8vgUeAvwUWgH/IzJfG26okaTVGnuFHxAxwBNgL7AAORMSOJWUHgTcy82+ArwP/Nu5GJUmr02VJ5wbgVGaezsy3gWPA/iU1+4HvDL/+HnBTRPTG16YkabW6LOlsAc4s2p4Hdl+qJjMvRMSbwPuA1y910A0beq/3+7MvX1m761e/P7vWLVw1nEXLWbScxf/765Xu2CXwlztTb1ZQs1S/w2dLksaky5LOPLBt0fZW4OylaiJiI3AN8LtxNChJGo8uZ/jPANsj4lrgFeBW4LYlNSeAfwSeAm4B/jczR53hS5KmaOQZfmZeAA4BJ4EXgeOZ+XxE3B8R+4Zl/wW8LyJOAXcB906qYUnSyvSaxhNxSarAO20lqQgDX5KK6PRohdXwsQytDrO4C/gMcAF4Dfh0Zq7LexVGzWJR3S3AY8BHMvPnU2xxarrMIiI+BdzH4HLn5zJz6YUT60KHn5EPMLjJc25Yc29mPj71RicsIr4NfBJ4NTN3LvN+j8GcPgGcB+7IzF+MOu5Ez/B9LEOr4yx+CezKzA8zuGP5wel2OR0dZ0FEzAJfAH423Q6np8ssImI78GXgY5n5IeCLU290Cjp+X3yVwYUj1zO4YvAb0+1yah4G9lzm/b3A9uGfzwLf7HLQSS/p+FiG1shZZOYTmXl+uPk0g3se1qMu3xcAX2PwS+/302xuyrrM4k7gSGa+AZCZr065x2npMosGeO/w62v403uC1oXM/DGXv5dpP/BIZjaZ+TQwFxHvH3XcSQf+co9l2HKpmuEloO88lmG96TKLxQ4CP5poR2tn5Cwi4npgW2b+cJqNrYEu3xfXAddFxE8j4unhssd61GUW9wG3R8Q88Djw+em0dtW50jwBJh/4k3osw5+jzv+dEXE7sAt4aKIdrZ3LziIiNjBY3rt7ah2tnS7fFxsZ/NX9RuAAcDQi5ibc11roMosDwMOZuZXB+vV3h98v1awoNyc9KB/L0OoyCyLiZuArwL7M/MOUepu2UbOYBXYCT0bES8BHgRMRsWtaDU5R15+RH2TmHzPz10Ay+AWw3nSZxUHgOEBmPgW8G9g8le6uLp3yZKlJX6XjYxlaI2cxXMb4FrBnHa/TwohZZOabLPohjogngXvW6VU6XX5Gvs/wzDYiNjNY4jk91S6no8ssfgPcxGAWH2QQ+K9NtcurwwngUEQcY/D04jcz87ejdproGb6PZWh1nMVDwHuAxyLiVxFxYo3anaiOsyih4yxOAgsR8QLwBPClzFxYm44np+Ms7gbujIjngEcZXI647k4QI+JRBifBERHzEXEwIj4XEZ8bljzO4Jf+KeA/gX/uclwfrSBJRVT8xw5JKsnAl6QiDHxJKsLAl6QiDHxJKsLAl6QiDHxJKuL/AObt/s30cB12AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Your code here\n",
    "dimensions = np.arange(1,10)\n",
    "avg_distances = []\n",
    "\n",
    "for dimension in dimensions:\n",
    "    random_numbers = np.random.uniform(-10, 10, 100 * dimension)\n",
    "    points = random_numbers.reshape(100,dimension)\n",
    "    origin = np.zeros((100,dimension))\n",
    "    distances_to_origin = [euclidean_distance(point, origin) for point in points]\n",
    "    avg_distances.append(np.mean(distances_to_origin))\n",
    "\n",
    "plt.plot(dimension, avg_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence Time\n",
    "\n",
    "As you've heard, another issue with increasing feature space is the training time required to fit a machine learning model. While more data will generally lead to better predictive results, it will also substantially increase training time. To demonstrate this, generate lists of random numbers as you did above. Then, use this list of random numbers as a feature in a mock dataset; choose an arbitrary coefficient and multiply the feature vector by this coefficient. Then sum these feature-coefficient products to get an output y. To spice things up (and not have a completely deterministic relationship), add a normally distributed white noise parameter to your output values. Fit an ordinary least squares model to your generated mock data. Repeat this for a varying number of features, and record the time required to fit the model. (Be sure to only record the time to train the model, not the time to generate the data.) Finally, plot the number of features, n, versus the training time for the subsequent model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn.linear_model import LinearRegression, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat the Same Experiment for a Lasso Penalized Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Show Just How Slow it Can Go!\n",
    "\n",
    "If you're up for putting your computer through the ringer and are very patient to allow the necessary computations, try increasing the maximum n from 1000 to 10,000 using Lasso regression. You should see an interesting pattern unveil. See if you can make any hypotheses as to why this might occur!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you conducted various simulations to investigate the curse of dimensionality. This demonstrated some of the caveats of working with large datasets with an increasing number of features. With that, the next section will start to explore Primary Component Analysis, a means of reducing the number of features in a dataset while preserving as much information as possible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
